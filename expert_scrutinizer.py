from fastmcp import FastMCP
from bs4 import BeautifulSoup
import re
import json

# -----------------------------------------------------------------------------
# Korean Jamo (ìëª¨) Decomposition Utilities
# LLM-style tokenization: í•œê¸€ â†’ ì´ˆì„±/ì¤‘ì„±/ì¢…ì„± ë¶„í•´
# -----------------------------------------------------------------------------

# Unicode Hangul Jamo
CHOSUNG = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
JUNGSUNG = ['ã…', 'ã…', 'ã…‘', 'ã…’', 'ã…“', 'ã…”', 'ã…•', 'ã…–', 'ã…—', 'ã…˜', 'ã…™', 'ã…š', 'ã…›', 'ã…œ', 'ã…', 'ã…', 'ã…Ÿ', 'ã… ', 'ã…¡', 'ã…¢', 'ã…£']
JONGSUNG = ['', 'ã„±', 'ã„²', 'ã„³', 'ã„´', 'ã„µ', 'ã„¶', 'ã„·', 'ã„¹', 'ã„º', 'ã„»', 'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€', 'ã…', 'ã…‚', 'ã…„', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']

def decompose_hangul(text: str) -> str:
    """
    Decompose Korean text into jamo (ì´ˆì„±/ì¤‘ì„±/ì¢…ì„±).
    Example: "ë”ë¼" â†’ "ã„·ã…“ã„¹ã…"
    Non-Hangul characters are kept as-is.
    """
    result = []
    for char in text:
        code = ord(char)
        # Hangul syllable range: 0xAC00 ~ 0xD7A3
        if 0xAC00 <= code <= 0xD7A3:
            offset = code - 0xAC00
            cho = offset // (21 * 28)
            jung = (offset % (21 * 28)) // 28
            jong = offset % 28
            result.append(CHOSUNG[cho])
            result.append(JUNGSUNG[jung])
            if jong > 0:
                result.append(JONGSUNG[jong])
        else:
            result.append(char)
    return ''.join(result)

def compose_jamo(jamo_str: str) -> str:
    """
    Compose jamo back into Hangul syllables.
    Example: "ã„·ã…“ã„¹ã…" â†’ "ë”ë¼"
    """
    result = []
    i = 0
    while i < len(jamo_str):
        char = jamo_str[i]
        # Check if it's a chosung
        if char in CHOSUNG and i + 1 < len(jamo_str) and jamo_str[i + 1] in JUNGSUNG:
            cho = CHOSUNG.index(char)
            jung = JUNGSUNG.index(jamo_str[i + 1])
            jong = 0
            i += 2
            # Check for jongsung
            if i < len(jamo_str) and jamo_str[i] in JONGSUNG[1:]:
                # Peek ahead to see if this is actually a chosung for next syllable
                if i + 1 < len(jamo_str) and jamo_str[i + 1] in JUNGSUNG:
                    pass  # It's a chosung, don't consume
                else:
                    jong = JONGSUNG.index(jamo_str[i])
                    i += 1
            syllable = chr(0xAC00 + (cho * 21 * 28) + (jung * 28) + jong)
            result.append(syllable)
        else:
            result.append(char)
            i += 1
    return ''.join(result)

# Define the MCP Server
mcp = FastMCP("Blog Scrutinizer")

# -----------------------------------------------------------------------------
# 0. Persona Context Manager
# -----------------------------------------------------------------------------

def _manage_persona_context(persona_input: dict = None) -> str:
    """
    Manages the persona context. If no input is provided, returns empty fields.
    Does NOT guess or generate heuristics.
    """
    if not persona_input:
        persona = {
            "writer": {"expertise": "", "mental_state": "", "tone": "", "image_strategy": ""},
            "reader": {"background": "", "needs": "", "intellectual_level": ""}
        }
    else:
        # Use provided input
        persona = persona_input

    # Ensure no names are concatenated in labels
    return json.dumps(persona, ensure_ascii=False, indent=2)

# -----------------------------------------------------------------------------
# 1. Hybrid Linguistic Auditor
# -----------------------------------------------------------------------------

def _audit_linguistic_quality(content: str, persona_json: str = "{}") -> str:
    """
    Provides Engineering-based audit for linguistic structure:
    1. Syntactic Complexity (Ratio > 1.5)
    2. Stale Word Blocking (Hard Fail)
    3. Sharp Word Statistics (Guidance for LLM)
    """
    soup = BeautifulSoup(content, 'html.parser')
    text_content = soup.get_text()
    
    report = []
    errors = 0

    # A. ì ˆ(Clause) / êµ¬(Phrase) ë¶„ì„
    # - ì ˆ: ìëª¨ ê¸°ë°˜ ì¢…ê²°ì–´ë¯¸ + ê´€í˜•ì ˆ(ì•ˆì€ë¬¸ì¥) ìë™ ê³„ì‚°
    # - êµ¬: LLMì´ ì§ì ‘ ì„¸ê³  ë¬´ì—‡ì„ ì…ŒëŠ”ì§€ ë³‘ê¸° (ëª…ì‚¬êµ¬/ë™ì‚¬êµ¬/ë¶€ì‚¬êµ¬)
    # - ëª©í‘œ ë¹„ìœ¨: ì ˆ:êµ¬ = 2:1 (Â±0.3 ì˜¤ì°¨ í—ˆìš©)
    
    jamo_text = decompose_hangul(text_content)
    
    # 1. ì¢…ê²°ì–´ë¯¸ ê°ì§€ (ë…ë¦½ì ˆ)
    terminal_patterns = [
        r'ã„·ã…[\.!]',  # -ë‹¤
        r'ã…‡ã…“[\.!]', r'ã…‡ã…[\.!]',  # -ì–´/-ì•„
        r'ã…ˆã…£[\.!]',  # -ì§€
        r'ã„´ã…”[\.!]',  # -ë„¤
        r'ã…‡ã…›[\.!]',  # -ìš” ê³„ì—´
        r'ã„´ã…£ã„·ã…[\.!]',  # -ë‹ˆë‹¤
        r'ã„´ã…‘[\?]', r'ã„´ã…£[\?]',  # -ëƒ/-ë‹ˆ?
        r'ã…ˆã…[\.!]',  # -ì
        r'ã„¹ã…[\.!]',  # -ë¼
    ]
    terminal_clause_count = sum(len(re.findall(p, jamo_text)) for p in terminal_patterns)
    
    # 2. ê´€í˜•ì ˆ/ì•ˆì€ë¬¸ì¥ ê°ì§€ (ë‚´í¬ì ˆ)
    # ê´€í˜•ì‚¬í˜• ì–´ë¯¸: -ëŠ”, -ã„´/ì€, -ã„¹/ì„, -ë˜ + ì²´ì–¸
    embedded_markers = re.findall(r'(?:ëŠ”|ã„´|ì€|ã„¹|ì„|ë˜)\s*(?:ê²ƒ|ê±°|ë•Œ|ê³³|ì´|ìˆ˜|ì¤„|ë²•|ë¦¬)', text_content)
    embedded_clause_count = len(embedded_markers)
    
    # 3. ì´ ì ˆ ìˆ˜
    total_clause_count = terminal_clause_count + embedded_clause_count
    
    report.append(f"â„¹ï¸ [ì ˆ-Stats] ì¢…ê²°ì–´ë¯¸(ë…ë¦½ì ˆ): {terminal_clause_count}ê°œ")
    report.append(f"â„¹ï¸ [ì ˆ-Stats] ê´€í˜•ì ˆ(ì•ˆì€ë¬¸ì¥): {embedded_clause_count}ê°œ")
    report.append(f"â„¹ï¸ [ì ˆ-Total] ì´ ì ˆ ìˆ˜: {total_clause_count}ê°œ (ì‹œìŠ¤í…œ ìë™ ê³„ì‚°)")
    
    # 4. êµ¬(Phrase) ê°€ì´ë“œ - LLMì´ ì§ì ‘ ì„¸ê³  ë³‘ê¸° (ëª©í‘œ ë¹„ìœ¨ì€ ìˆ¨ê¹€)
    # êµ¬ = ìˆ ì–´ ê¸°ëŠ¥ì´ ì—†ëŠ” ë‹¨ìœ„ (ëª…ì‚¬êµ¬, ë™ì‚¬êµ¬, ë¶€ì‚¬êµ¬ + ë…ë¦½ ë¶€ì‚¬/ê´€í˜•ì‚¬)
    report.append(f"\nğŸ“‹ [êµ¬-Task] LLMì´ ì§ì ‘ 'êµ¬' ë‹¨ìœ„ë¥¼ ì„¸ê³  ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë³´ê³ :")
    report.append(f"   (êµ¬ = ìˆ ì–´ ê¸°ëŠ¥ ì—†ëŠ” ë‹¨ìœ„. ë¶€ì‚¬/ê´€í˜•ì‚¬ ë‹¨ë…ë„ í¬í•¨)")
    report.append(f"   â†’ ëª…ì‚¬êµ¬(NP): [ì‹¤ì œ ë°œê²¬í•œ êµ¬ ë‚˜ì—´] â†’ ì´ ___ê°œ")
    report.append(f"   â†’ ë™ì‚¬êµ¬(VP): [ì‹¤ì œ ë°œê²¬í•œ êµ¬ ë‚˜ì—´] â†’ ì´ ___ê°œ")
    report.append(f"   â†’ ë¶€ì‚¬êµ¬(AP): [ì‹¤ì œ ë°œê²¬í•œ êµ¬ ë‚˜ì—´] â†’ ì´ ___ê°œ")
    report.append(f"   â†’ ë…ë¦½ë¶€ì‚¬: [ì˜ˆ: 'ë‹¨ì–¸ì»¨ëŒ€', 'ì••ë„ì ìœ¼ë¡œ', 'ê·¸ëŸ¬ë‚˜'] â†’ ì´ ___ê°œ")
    report.append(f"   â†’ ê´€í˜•ì‚¬: [ì˜ˆ: 'ì´', 'ê·¸', 'ìƒˆë¡œìš´', 'ëª¨ë“ '] â†’ ì´ ___ê°œ")
    report.append(f"   â†’ êµ¬ ì´ê³„: ___ê°œ")
    report.append(f"âš ï¸ [ì£¼ì˜] ë°˜ë“œì‹œ ë°œê²¬í•œ êµ¬ë¥¼ ì‹¤ì œë¡œ ë‚˜ì—´í•  ê²ƒ. ìˆ«ìë§Œ ë³´ê³  ê¸ˆì§€.")

    # B. Lexical Audit (Engineering)
    # 1. Hard Block: Stale Words
    # Removed 'ì•Œì•„ë³´ì' as per instruction to leave it to LLM judgment within Reckon Vibe
    stale_words = ['ì‚¬ë£Œëœë‹¤', 'ê³ ì°°', 'ë³¸ì¸', 'í•˜ì˜€ìŒ', 'ì˜ë¯¸í•œë‹¤', 'ëœ»í•œë‹¤']
    found_stale = [w for w in stale_words if w in text_content]
    if found_stale:
        report.append(f"âŒ [Lexical-Block] Stale words detected: {found_stale}. Replace with dynamic/sharp alternatives.")
        errors += 1
    
    # 2. Guidance: Sharp Word Statistics
    sharp_pool = ['ì¥ì•…', 'ì„¤ê³„', 'ì••ë„ì ', 'ë©”ì»¤ë‹ˆì¦˜', 'ë‹¨ì–¸ì»¨ëŒ€', 'ê·€ê²°', 'ë‚©ë“', 'ì–‘ìƒ']
    found_sharp = [w for w in sharp_pool if w in text_content]
    report.append(f"â„¹ï¸ [Lexical-Stats] Sharp word count: {len(found_sharp)}. (Examples found: {found_sharp})")
    
    # C. Korean Speech Register (ì–´ì²´) Audit - JAMO-BASED SYSTEM
    # Level hierarchy (low â†’ high): í•´ë¼ < í•´ < í•˜ê²Œ < í•˜ì˜¤ < í•´ìš” < í•˜ì‹­ì‹œì˜¤
    # 
    # JAMO DECOMPOSITION: í•œê¸€ â†’ ã…ˆ/ã…/ã…/ã…— ë‹¨ìœ„ ë¶„í•´
    # Example: "ë”ë¼" â†’ "ã„·ã…“ã„¹ã…", "í–ˆë‹¤" â†’ "ã…ã…ã…†ã„·ã…"
    #
    # PATTERNS INCLUDE:
    # - ë³¸ìš©ì–¸ ì¢…ê²°ì–´ë¯¸ (main verb endings)
    # - ë³´ì¡°ìš©ì–¸ ê²°í•©í˜• (auxiliary verb combinations)
    #   - ~ì•„/ì–´ ë³´ë‹¤, ì£¼ë‹¤, ë²„ë¦¬ë‹¤, ë†“ë‹¤, ë‘ë‹¤, ê°€ë‹¤, ì˜¤ë‹¤, ë‚´ë‹¤, ëŒ€ë‹¤
    #   - ~ê³  ìˆë‹¤, ì‹¶ë‹¤, ë§ë‹¤
    #   - ~ì•„/ì–´ì•¼ í•˜ë‹¤/ë˜ë‹¤
    #   - ~ã„¹/ì„ ìˆ˜ ìˆë‹¤/ì—†ë‹¤
    # - ì‹œì œ/ìƒ ê²°í•© (tense/aspect: ì•˜/ì—ˆ/ê² )
    # - ë¶ˆê·œì¹™ í™œìš© (irregular: ã„¹/ã…‚/ã„·/ã……/ã…)
    # - ë¹„í‘œì¤€/ë°©ì–¸/ì¶•ì•½í˜•
    
    # Convert text to jamo for pattern matching
    jamo_text = decompose_hangul(text_content)
    
    # Jamo building blocks (for constructing patterns)
    # ëª¨ìŒì¡°í™” ê·¸ë£¹
    YANG_V = 'ã…ã…—ã…‘ã…›'  # ì–‘ì„±ëª¨ìŒ
    YIN_V = 'ã…“ã…œã…•ã… ã…¡ã…£ã…ã…”ã…šã…Ÿ'  # ìŒì„±ëª¨ìŒ
    ALL_V = YANG_V + YIN_V + 'ã…˜ã…™ã…ã…ã…¢ã…’ã…–'  # ëª¨ë“  ëª¨ìŒ
    
    # Common auxiliary verb stems in jamo (ë³´ì¡°ìš©ì–¸ ì–´ê°„)
    # ë³´ë‹¤(ã…‚ã…—ã„·ã…), ì£¼ë‹¤(ã…ˆã…œã„·ã…), ë²„ë¦¬ë‹¤(ã…‚ã…“ã„¹ã…£ã„·ã…), ë†“ë‹¤(ã„´ã…—ã…ã„·ã…), ë‘ë‹¤(ã„·ã…œã„·ã…)
    # ê°€ë‹¤(ã„±ã…ã„·ã…), ì˜¤ë‹¤(ã…‡ã…—ã„·ã…), ë‚´ë‹¤(ã„´ã…ã„·ã…), ìˆë‹¤(ã…‡ã…£ã…†ã„·ã…), ì‹¶ë‹¤(ã……ã…£ã…ã„·ã…)
    
    # Optional preceding elements (tense/aspect markers in jamo)
    TENSE_OPT = r'(?:ã…‡ã…ã…†|ã…‡ã…“ã…†|ã„±ã…”ã…†)?'  # ì•˜/ì—ˆ/ê²  (optional)
    AUX_OPT = r'(?:ã…‚ã…—|ã…ˆã…œ|ã…‚ã…“ã„¹ã…£|ã„´ã…—ã…|ã„·ã…œ|ã„±ã…|ã…‡ã…—|ã„´ã…|ã…‡ã…£ã…†|ã……ã…£ã…)?'  # ë³´/ì£¼/ë²„ë¦¬/ë†“/ë‘/ê°€/ì˜¤/ë‚´/ìˆ/ì‹¶
    
    speech_levels = {
        'í•´ë¼ì²´': {
            'jamo_patterns': [
                # === í‰ì„œí˜• ===
                rf'{TENSE_OPT}ã„·ã…[\\.!]',  # -ë‹¤
                rf'{TENSE_OPT}ã„´ã…¡ã„´ã„·ã…[\\.!]',  # -ëŠ”ë‹¤
                rf'{TENSE_OPT}ã„´ã„·ã…[\\.!]',  # -ã„´ë‹¤
                rf'{TENSE_OPT}ã„·ã…“ã„¹ã…[\\.!]',  # -ë”ë¼
                rf'{TENSE_OPT}ã„·ã…¡ã„¹ã…[\\.!]',  # -ë“œë¼ (ë¹„í‘œì¤€)
                rf'{TENSE_OPT}ã„¹ã…ã„´ã„·ã…[\\.!]',  # -ë€ë‹¤
                rf'{TENSE_OPT}ã„·ã…ã„´ã„·ã…[\\.!]',  # -ë‹¨ë‹¤
                rf'{TENSE_OPT}ã„¹ã…£ã„¹ã…[\\.!]',  # -ë¦¬ë¼
                rf'{TENSE_OPT}ã„±ã…œã„´ã…[\\.!]',  # -êµ¬ë‚˜
                rf'{TENSE_OPT}ã„±ã…œã„´[\\.!]',  # -êµ°
                # ë³´ì¡°ìš©ì–¸ ê²°í•©: ~ì•„/ì–´ ë´¤ë‹¤, ì¤¬ë‹¤, ë²„ë ¸ë‹¤, etc.
                rf'ã…‚ã…—{TENSE_OPT}ã„·ã…[\\.!]',  # ë´¤ë‹¤/ë³¸ë‹¤
                rf'ã…ˆã…œ{TENSE_OPT}ã„·ã…[\\.!]',  # ì¤¬ë‹¤/ì¤€ë‹¤  
                rf'ã…‚ã…“ã„¹ã…£{TENSE_OPT}ã„·ã…[\\.!]',  # ë²„ë ¸ë‹¤
                rf'ã„´ã…—ã…{TENSE_OPT}ã„·ã…[\\.!]',  # ë†¨ë‹¤
                rf'ã„·ã…œ{TENSE_OPT}ã„·ã…[\\.!]',  # ë’€ë‹¤
                rf'ã„±ã…{TENSE_OPT}ã„·ã…[\\.!]',  # ê°”ë‹¤
                rf'ã…‡ã…—{TENSE_OPT}ã„·ã…[\\.!]',  # ì™”ë‹¤
                # ~ê³  ìˆë‹¤, ì‹¶ë‹¤
                rf'ã„±ã…—ã…‡ã…£ã…†ã„·ã…[\\.!]',  # ê³  ìˆë‹¤
                rf'ã„±ã…—ã……ã…£ã…ã„·ã…[\\.!]',  # ê³  ì‹¶ë‹¤
                # === ì˜ë¬¸í˜• ===
                rf'{TENSE_OPT}ã„´ã…‘[\\?]',  # -ëƒ
                rf'{TENSE_OPT}ã„´ã…¡ã„´ã…‘[\\?]',  # -ëŠëƒ
                rf'{TENSE_OPT}ã„´ã…£[\\?]',  # -ë‹ˆ
                rf'{TENSE_OPT}ã„·ã…“ã„´ã…‘[\\?]',  # -ë”ëƒ
                rf'{TENSE_OPT}ã„¹ã„²ã…[\\?]',  # -ã„¹ê¹Œ
                rf'{TENSE_OPT}ã…‡ã…¡ã„¹ã„²ã…[\\?]',  # -ì„ê¹Œ
                # === ëª…ë ¹í˜• ===
                rf'ã…‡ã…ã„¹ã…[\\.!]',  # -ì•„ë¼
                rf'ã…‡ã…“ã„¹ã…[\\.!]',  # -ì–´ë¼
                rf'ã…‡ã…•ã„¹ã…[\\.!]',  # -ì—¬ë¼
                rf'ã„±ã…“ã„¹ã…[\\.!]',  # -ê±°ë¼
                rf'ã„´ã…“ã„¹ã…[\\.!]',  # -ë„ˆë¼
                rf'ã„¹ã…•ã…[\\.!]',  # -ë ´
                # === ì²­ìœ í˜• ===
                rf'ã…ˆã…[\\.!]',  # -ì
            ],
            'examples': {
                'í‰ì„œ': ['-ë‹¤', '-ã„´ë‹¤/ëŠ”ë‹¤', '-ì•˜/ì—ˆë‹¤', '-ë”ë¼', '-êµ¬ë‚˜', '-êµ°'],
                'ì˜ë¬¸': ['-ëƒ?', '-ëŠëƒ?', '-ë‹ˆ?', '-ã„¹ê¹Œ?'],
                'ëª…ë ¹': ['-ì•„ë¼/ì–´ë¼', '-ê±°ë¼', '-ë ´'],
                'ì²­ìœ ': ['-ì'],
                'ë³´ì¡°ìš©ì–¸': ['~ë´¤ë‹¤', '~ì¤¬ë‹¤', '~ë²„ë ¸ë‹¤', '~ë†¨ë‹¤', '~ê°”ë‹¤', '~ì™”ë‹¤', '~ê³  ìˆë‹¤', '~ê³  ì‹¶ë‹¤']
            },
            'level': 1
        },
        'í•´ì²´': {
            'jamo_patterns': [
                # === í‰ì„œí˜• === (~ì–´/ì•„/ì—¬ ì¢…ê²°)
                rf'{TENSE_OPT}ã…‡ã…“[\\.!]',  # -ì–´
                rf'{TENSE_OPT}ã…‡ã…[\\.!]',  # -ì•„
                rf'{TENSE_OPT}ã…‡ã…•[\\.!]',  # -ì—¬
                rf'{TENSE_OPT}ã…ˆã…£[\\.!]',  # -ì§€
                rf'{TENSE_OPT}ã„±ã…“ã„·ã…¡ã„´[\\.!]',  # -ê±°ë“ 
                rf'{TENSE_OPT}ã„´ã…”[\\.!]',  # -ë„¤
                rf'{TENSE_OPT}ã„´ã…¡ã„´ã„·ã…”[\\.!]',  # -ëŠ”ë°
                rf'{TENSE_OPT}ã…ˆã…ã„´ã…ã…[\\.!]',  # -ì–ì•„
                rf'{TENSE_OPT}ã„·ã…“ã„¹ã…ã„±ã…—[\\.!]',  # -ë”ë¼ê³ 
                rf'{TENSE_OPT}ã„¹ã„±ã…“ã„¹[\\.!]',  # -ã„¹ê±¸
                rf'{TENSE_OPT}ã„¹ã„±ã…”[\\.!]',  # -ã„¹ê²Œ
                # ë³´ì¡°ìš©ì–¸ ê²°í•©: ë´, ì¤˜, ë²„ë ¤, ë†”, ë‘¬
                rf'ã…‚ã…˜[\\.!]',  # ë´ (ë³´+ì•„â†’ë´)
                rf'ã…ˆã…[\\.!]',  # ì¤˜ (ì£¼+ì–´â†’ì¤˜)
                rf'ã…‚ã…“ã„¹ã…•[\\.!]',  # ë²„ë ¤
                rf'ã„´ã…˜[\\.!]',  # ë†” (ë†“+ì•„â†’ë†”)
                rf'ã„·ã…[\\.!]',  # ë‘¬ (ë‘+ì–´â†’ë‘¬)
                rf'ã„±ã…[\\.!]',  # ê°€
                rf'ã…‡ã…˜[\\.!]',  # ì™€ (ì˜¤+ì•„â†’ì™€)
                # ~ê³  ìˆì–´, ì‹¶ì–´
                rf'ã„±ã…—ã…‡ã…£ã…†ã…‡ã…“[\\.!]',  # ê³  ìˆì–´
                rf'ã„±ã…—ã……ã…£ã…ã…‡ã…“[\\.!]',  # ê³  ì‹¶ì–´
                # === ì˜ë¬¸í˜• ===
                rf'{TENSE_OPT}ã…‡ã…“[\\?]',  # -ì–´?
                rf'{TENSE_OPT}ã…‡ã…[\\?]',  # -ì•„?
                rf'{TENSE_OPT}ã…ˆã…£[\\?]',  # -ì§€?
                rf'{TENSE_OPT}ã„´ã…¡ã„´ã„·ã…”[\\?]',  # -ëŠ”ë°?
            ],
            'examples': {
                'í‰ì„œ': ['-ì–´/ì•„', '-ì§€', '-ê±°ë“ ', '-ë„¤', '-ëŠ”ë°', '-ì–ì•„', '-ë”ë¼ê³ ', '-ã„¹ê±¸', '-ã„¹ê²Œ'],
                'ì˜ë¬¸': ['-ì–´?/ì•„?', '-ì§€?', '-ëŠ”ë°?', '-ì–ì•„?'],
                'ëª…ë ¹': ['-ì–´/ì•„ (í‰ì„œì™€ ë™ì¼)'],
                'ì²­ìœ ': ['-ì–´/ì•„ (í‰ì„œì™€ ë™ì¼)'],
                'ë³´ì¡°ìš©ì–¸': ['~ë´', '~ì¤˜', '~ë²„ë ¤', '~ë†”', '~ë‘¬', '~ê°€', '~ì™€', '~ê³  ìˆì–´', '~ê³  ì‹¶ì–´']
            },
            'level': 2
        },
        'í•˜ê²Œì²´': {
            'jamo_patterns': [
                # === í‰ì„œí˜• ===
                rf'{TENSE_OPT}ã„´ã…”[\\.!]',  # -ë„¤ (ì¤‘ë³µ í—ˆìš©)
                rf'{TENSE_OPT}ã„±ã…”ã…†ã„´ã…”[\\.!]',  # -ê² ë„¤
                rf'{TENSE_OPT}ã„´ã„±ã…[\\.!]',  # -ã„´ê°€
                rf'{TENSE_OPT}ã„´ã…¡ã„´ã„±ã…[\\.!]',  # -ëŠ”ê°€
                # === ì˜ë¬¸í˜• ===
                rf'{TENSE_OPT}ã„´ã…[\\?]',  # -ë‚˜?
                rf'{TENSE_OPT}ã„´ã…¡ã„´ã„±ã…[\\?]',  # -ëŠ”ê°€?
                rf'{TENSE_OPT}ã„·ã…“ã„´ã„±ã…[\\?]',  # -ë˜ê°€?
                # === ëª…ë ¹í˜• ===
                rf'ã„±ã…”[\\.!]',  # -ê²Œ
                rf'ã„±ã…”ã„´ã…[\\.!]',  # -ê²Œë‚˜
                # === ì²­ìœ í˜• ===
                rf'ã……ã…”[\\.!]',  # -ì„¸
                rf'ã……ã…”ã„´ã…[\\.!]',  # -ì„¸ë‚˜
            ],
            'examples': {
                'í‰ì„œ': ['-ë„¤', '-ê² ë„¤', '-ã„´ê°€/ëŠ”ê°€'],
                'ì˜ë¬¸': ['-ë‚˜?', '-ëŠ”ê°€?', '-ë˜ê°€?'],
                'ëª…ë ¹': ['-ê²Œ', '-ê²Œë‚˜'],
                'ì²­ìœ ': ['-ì„¸', '-ì„¸ë‚˜']
            },
            'level': 3
        },
        'í•˜ì˜¤ì²´': {
            'jamo_patterns': [
                # === í‰ì„œí˜• ===
                rf'{TENSE_OPT}ã…‡ã…—[\\.!]',  # -ì˜¤
                rf'{TENSE_OPT}ã……ã…—[\\.!]',  # -ì†Œ
                rf'{TENSE_OPT}ã„¹ã…£ã…‡ã…—[\\.!]',  # -ë¦¬ì˜¤
                rf'{TENSE_OPT}ã„±ã…œã„¹ã…•[\\.!]',  # -êµ¬ë ¤
                # === ì˜ë¬¸í˜• ===
                rf'{TENSE_OPT}ã…‡ã…—[\\?]',  # -ì˜¤?
                rf'{TENSE_OPT}ã……ã…—[\\?]',  # -ì†Œ?
                # === ëª…ë ¹í˜• ===
                rf'ã……ã…£ã…‡ã…—[\\.!]',  # -ì‹œì˜¤
                # === ì²­ìœ í˜• ===
                rf'ã…‚ã……ã…£ã„·ã…[\\.!]',  # -ã…‚ì‹œë‹¤
                rf'ã…ã…ã…‚ã……ã…£ã„·ã…[\\.!]',  # í•©ì‹œë‹¤
                rf'ã„±ã…ã…‚ã……ã…£ã„·ã…[\\.!]',  # ê°‘ì‹œë‹¤
                rf'ã…‚ã…—ã…‚ã……ã…£ã„·ã…[\\.!]',  # ë´…ì‹œë‹¤
            ],
            'examples': {
                'í‰ì„œ': ['-ì˜¤', '-ì†Œ', '-ë¦¬ì˜¤', '-êµ¬ë ¤'],
                'ì˜ë¬¸': ['-ì˜¤?', '-ì†Œ?'],
                'ëª…ë ¹': ['-ì‹œì˜¤'],
                'ì²­ìœ ': ['-ã…‚ì‹œë‹¤', 'í•©ì‹œë‹¤', 'ê°‘ì‹œë‹¤', 'ë´…ì‹œë‹¤']
            },
            'level': 4
        },
        'í•´ìš”ì²´': {
            'jamo_patterns': [
                # === í‰ì„œí˜• ===
                rf'{TENSE_OPT}ã…‡ã…“ã…‡ã…›[\\.!]',  # -ì–´ìš”
                rf'{TENSE_OPT}ã…‡ã…ã…‡ã…›[\\.!]',  # -ì•„ìš”
                rf'{TENSE_OPT}ã…‡ã…•ã…‡ã…›[\\.!]',  # -ì—¬ìš”
                rf'{TENSE_OPT}ã…‡ã…”ã…‡ã…›[\\.!]',  # -ì—ìš”
                rf'{TENSE_OPT}ã…‡ã…–ã…‡ã…›[\\.!]',  # -ì˜ˆìš”
                rf'{TENSE_OPT}ã…ˆã…›[\\.!]',  # -ì£ 
                rf'{TENSE_OPT}ã…ˆã…£ã…‡ã…›[\\.!]',  # -ì§€ìš”
                rf'{TENSE_OPT}ã„´ã…”ã…‡ã…›[\\.!]',  # -ë„¤ìš”
                rf'{TENSE_OPT}ã„±ã…œã„´ã…‡ã…›[\\.!]',  # -êµ°ìš”
                rf'{TENSE_OPT}ã„±ã…“ã„·ã…¡ã„´ã…‡ã…›[\\.!]',  # -ê±°ë“ ìš”
                rf'{TENSE_OPT}ã…ˆã…ã„´ã…ã…ã…‡ã…›[\\.!]',  # -ì–ì•„ìš”
                rf'{TENSE_OPT}ã„´ã…¡ã„´ã„·ã…”ã…‡ã…›[\\.!]',  # -ëŠ”ë°ìš”
                rf'{TENSE_OPT}ã„¹ã„±ã…”ã…‡ã…›[\\.!]',  # -ã„¹ê²Œìš”
                rf'{TENSE_OPT}ã„¹ã„²ã…“ã„¹ã…‡ã…›[\\.!]',  # -ã„¹ê±¸ìš”
                # ë³´ì¡°ìš©ì–¸: ë´ìš”, ì¤˜ìš”, ë²„ë ¤ìš”
                rf'ã…‚ã…˜ã…‡ã…›[\\.!]',  # ë´ìš”
                rf'ã…ˆã…ã…‡ã…›[\\.!]',  # ì¤˜ìš”
                rf'ã…‚ã…“ã„¹ã…•ã…‡ã…›[\\.!]',  # ë²„ë ¤ìš”
                rf'ã„´ã…˜ã…‡ã…›[\\.!]',  # ë†”ìš”
                rf'ã„·ã…ã…‡ã…›[\\.!]',  # ë‘¬ìš”
                # ~ê³  ìˆì–´ìš”, ì‹¶ì–´ìš”
                rf'ã„±ã…—ã…‡ã…£ã…†ã…‡ã…“ã…‡ã…›[\\.!]',  # ê³  ìˆì–´ìš”
                rf'ã„±ã…—ã……ã…£ã…ã…‡ã…“ã…‡ã…›[\\.!]',  # ê³  ì‹¶ì–´ìš”
                # === ì˜ë¬¸í˜• ===
                rf'{TENSE_OPT}ã…‡ã…“ã…‡ã…›[\\?]',  # -ì–´ìš”?
                rf'{TENSE_OPT}ã…‡ã…ã…‡ã…›[\\?]',  # -ì•„ìš”?
                rf'{TENSE_OPT}ã…ˆã…›[\\?]',  # -ì£ ?
                rf'{TENSE_OPT}ã„´ã…ã…‡ã…›[\\?]',  # -ë‚˜ìš”?
                rf'{TENSE_OPT}ã„¹ã„²ã…ã…‡ã…›[\\?]',  # -ã„¹ê¹Œìš”?
                rf'{TENSE_OPT}ã„¹ã„¹ã…ã…‡ã…›[\\?]',  # -ã„¹ë˜ìš”?
                # === ëª…ë ¹í˜• ===
                rf'ã……ã…”ã…‡ã…›[\\.!]',  # -ì„¸ìš”
                rf'ã…ˆã…œã……ã…”ã…‡ã…›[\\.!]',  # -ì£¼ì„¸ìš”
            ],
            'examples': {
                'í‰ì„œ': ['-ì–´ìš”/ì•„ìš”', '-ì—ìš”/ì˜ˆìš”', '-ì£ ', '-ì§€ìš”', '-ë„¤ìš”', '-êµ°ìš”', '-ê±°ë“ ìš”', '-ì–ì•„ìš”', '-ëŠ”ë°ìš”', '-ã„¹ê²Œìš”', '-ã„¹ê±¸ìš”'],
                'ì˜ë¬¸': ['-ì–´ìš”?/ì•„ìš”?', '-ì£ ?', '-ë‚˜ìš”?', '-ã„¹ê¹Œìš”?', '-ã„¹ë˜ìš”?'],
                'ëª…ë ¹': ['-ì„¸ìš”', '-ì£¼ì„¸ìš”'],
                'ì²­ìœ ': ['-ì–´ìš”/ì•„ìš”', '-ã„¹ë˜ìš”'],
                'ë³´ì¡°ìš©ì–¸': ['~ë´ìš”', '~ì¤˜ìš”', '~ë²„ë ¤ìš”', '~ë†”ìš”', '~ë‘¬ìš”', '~ê³  ìˆì–´ìš”', '~ê³  ì‹¶ì–´ìš”']
            },
            'level': 5
        },
        'í•˜ì‹­ì‹œì˜¤ì²´': {
            'jamo_patterns': [
                # === í‰ì„œí˜• ===
                rf'{TENSE_OPT}ã……ã…¡ã…‚ã„´ã…£ã„·ã…[\\.!]',  # -ìŠµë‹ˆë‹¤
                rf'{TENSE_OPT}ã…‚ã„´ã…£ã„·ã…[\\.!]',  # -ã…‚ë‹ˆë‹¤
                rf'{TENSE_OPT}ã…‡ã…—ã…‚ã„´ã…£ã„·ã…[\\.!]',  # -ì˜µë‹ˆë‹¤
                rf'{TENSE_OPT}ã…‡ã…£ã…‚ã„´ã…£ã„·ã…[\\.!]',  # -ì…ë‹ˆë‹¤
                rf'{TENSE_OPT}ã„±ã…”ã…†ã……ã…¡ã…‚ã„´ã…£ã„·ã…[\\.!]',  # -ê² ìŠµë‹ˆë‹¤
                rf'{TENSE_OPT}ã…‡ã…“ã…†ã……ã…¡ã…‚ã„´ã…£ã„·ã…[\\.!]',  # -ì—ˆìŠµë‹ˆë‹¤
                rf'{TENSE_OPT}ã…‡ã…ã…†ã……ã…¡ã…‚ã„´ã…£ã„·ã…[\\.!]',  # -ì•˜ìŠµë‹ˆë‹¤
                # === ì˜ë¬¸í˜• ===
                rf'{TENSE_OPT}ã……ã…¡ã…‚ã„´ã…£ã„²ã…[\\?]',  # -ìŠµë‹ˆê¹Œ?
                rf'{TENSE_OPT}ã…‚ã„´ã…£ã„²ã…[\\?]',  # -ã…‚ë‹ˆê¹Œ?
                rf'{TENSE_OPT}ã…‡ã…£ã…‚ã„´ã…£ã„²ã…[\\?]',  # -ì…ë‹ˆê¹Œ?
                rf'{TENSE_OPT}ã……ã…£ã…‚ã„´ã…£ã„²ã…[\\?]',  # -ì‹­ë‹ˆê¹Œ?
                # === ëª…ë ¹í˜• ===
                rf'ã……ã…£ã…‚ã……ã…£ã…‡ã…—[\\.!]',  # -ì‹­ì‹œì˜¤
                rf'ã……ã…—ã……ã…“[\\.!]',  # -ì†Œì„œ (ê·¹ì¡´ì¹­)
                # === ì²­ìœ í˜• ===
                rf'ã……ã…£ã…‚ã……ã…£ã„·ã…[\\.!]',  # -ì‹­ì‹œë‹¤
            ],
            'examples': {
                'í‰ì„œ': ['-ìŠµë‹ˆë‹¤/ã…‚ë‹ˆë‹¤', '-ì…ë‹ˆë‹¤', '-ê² ìŠµë‹ˆë‹¤', '-ì•˜/ì—ˆìŠµë‹ˆë‹¤'],
                'ì˜ë¬¸': ['-ìŠµë‹ˆê¹Œ?/ã…‚ë‹ˆê¹Œ?', '-ì…ë‹ˆê¹Œ?', '-ì‹­ë‹ˆê¹Œ?'],
                'ëª…ë ¹': ['-ì‹­ì‹œì˜¤', '-ì†Œì„œ'],
                'ì²­ìœ ': ['-ì‹­ì‹œë‹¤']
            },
            'level': 6
        }
    }
    
    # Count occurrences per level using JAMO patterns on JAMO-decomposed text
    level_counts = {}
    for name, info in speech_levels.items():
        count = 0
        for pattern in info['jamo_patterns']:
            count += len(re.findall(pattern, jamo_text))
        level_counts[name] = {'count': count, 'level': info['level']}
    
    total_endings = sum(lc['count'] for lc in level_counts.values())
    
    if total_endings > 0:
        # Find dominant (primary) speech level
        sorted_levels = sorted(level_counts.items(), key=lambda x: x[1]['count'], reverse=True)
        primary_name, primary_info = sorted_levels[0]
        primary_count = primary_info['count']
        
        # Calculate primary ratio
        primary_ratio = primary_count / total_endings
        
        # Count non-primary endings
        other_endings = []
        for name, info in level_counts.items():
            if name != primary_name and info['count'] > 0:
                other_endings.append(f"{name}({info['count']})")
        
        report.append(f"â„¹ï¸ [ì–´ì²´-Stats] Detected: {primary_name}({primary_count}) dominant.")
        report.append(f"â„¹ï¸ [ì–´ì²´-Ratio] Primary {primary_ratio:.1%} of total ({total_endings} endings)")
        
        # SINGLE SPEECH REGISTER ENFORCEMENT (90%+ required, 10% tolerance)
        if primary_ratio >= 0.90:
            report.append(f"âœ… [ì–´ì²´] ë‹¨ì¼ ì–´ì²´({primary_name}) ì¼ê´€ì„± ìœ ì§€ë¨ ({primary_ratio:.1%}).")
        else:
            report.append(f"âŒ [ì–´ì²´] ì–´ì²´ í˜¼ìš© ê°ì§€! {primary_name} {primary_ratio:.1%}ë§Œ ì‚¬ìš©. ë‹¨ì¼ ì–´ì²´(90%+)ë¡œ í†µì¼ í•„ìš”.")
            if other_endings:
                report.append(f"âš ï¸ [ì–´ì²´-í˜¼ìš©] ë‹¤ë¥¸ ì–´ì²´ ê°ì§€: {', '.join(other_endings)}")
                report.append(f"ğŸ“‹ [ìˆ˜ì • ì§€ì‹œ] ëª¨ë“  ì¢…ê²°ì–´ë¯¸ë¥¼ '{primary_name}'ë¡œ í†µì¼í•˜ì„¸ìš”.")
                
                # Show examples of the primary speech level's endings
                primary_examples = speech_levels[primary_name].get('examples', {})
                report.append(f"\nğŸ“‹ [ì–´ì²´-ê°€ì´ë“œ] '{primary_name}' í—ˆìš© ì¢…ê²°ì–´ë¯¸:")
                for category, endings in primary_examples.items():
                    report.append(f"   â€¢ {category}: {', '.join(endings)}")
            errors += 1

    report.append("\nğŸ’¡ [Linguistic Note] Use the above structural data to apply your high-level linguistic judgment (Arousal, Reckon Vibe).")
    
    return "\n".join(report)

def _validate_clause_phrase_ratio(content: str, llm_phrase_count: int, llm_phrase_list: str = "") -> str:
    """
    LLMì´ ë³´ê³ í•œ êµ¬ ìˆ˜ë¥¼ ë°›ì•„ ì ˆ:êµ¬ ë¹„ìœ¨ ê²€ì¦.
    ëª©í‘œ: ì ˆ:êµ¬ = 2:1 (Â±0.3 ì˜¤ì°¨ í—ˆìš©)
    
    Args:
        content: HTML ì½˜í…ì¸ 
        llm_phrase_count: LLMì´ ì„¼ êµ¬ì˜ ì´ ê°œìˆ˜
        llm_phrase_list: LLMì´ ë‚˜ì—´í•œ êµ¬ ëª©ë¡ (ê²€ì¦ìš©)
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë³´ê³ ì„œ
    """
    soup = BeautifulSoup(content, 'html.parser')
    text_content = soup.get_text()
    jamo_text = decompose_hangul(text_content)
    
    report = []
    
    # 1. ì ˆ ìˆ˜ ì¬ê³„ì‚° (ì‹œìŠ¤í…œ)
    terminal_patterns = [
        r'ã„·ã…[\.!]', r'ã…‡ã…“[\.!]', r'ã…‡ã…[\.!]', r'ã…ˆã…£[\.!]',
        r'ã„´ã…”[\.!]', r'ã…‡ã…›[\.!]', r'ã„´ã…£ã„·ã…[\.!]',
        r'ã„´ã…‘[\?]', r'ã„´ã…£[\?]', r'ã…ˆã…[\.!]', r'ã„¹ã…[\.!]',
    ]
    terminal_count = sum(len(re.findall(p, jamo_text)) for p in terminal_patterns)
    
    embedded_markers = re.findall(r'(?:ëŠ”|ã„´|ì€|ã„¹|ì„|ë˜)\s*(?:ê²ƒ|ê±°|ë•Œ|ê³³|ì´|ìˆ˜|ì¤„|ë²•|ë¦¬)', text_content)
    embedded_count = len(embedded_markers)
    
    total_clause = terminal_count + embedded_count
    
    # 2. ë¹„ìœ¨ ê³„ì‚° ë° ê²€ì¦
    if llm_phrase_count > 0:
        ratio = total_clause / llm_phrase_count
    else:
        ratio = float('inf')
    
    target_ratio = 2.0
    tolerance = 0.3
    
    report.append(f"=== ì ˆ:êµ¬ ë¹„ìœ¨ ê²€ì¦ ===")
    report.append(f"ğŸ“Š [ì‹œìŠ¤í…œ] ì ˆ ìˆ˜: {total_clause}ê°œ")
    report.append(f"ğŸ“Š [LLM ë³´ê³ ] êµ¬ ìˆ˜: {llm_phrase_count}ê°œ")
    report.append(f"ğŸ“Š [ë¹„ìœ¨] ì ˆ:êµ¬ = {ratio:.2f}:1")
    
    if llm_phrase_list:
        report.append(f"ğŸ“‹ [LLM ë‚˜ì—´] {llm_phrase_list[:200]}...")  # 200ìê¹Œì§€ë§Œ
    
    # 3. íŒì •
    if target_ratio - tolerance <= ratio <= target_ratio + tolerance:
        report.append(f"âœ… [PASS] ì ˆ:êµ¬ ë¹„ìœ¨ ì í•© (ëª©í‘œ: 2:1 Â±0.3)")
    else:
        report.append(f"âŒ [FAIL] ì ˆ:êµ¬ ë¹„ìœ¨ ë¶€ì í•© (í˜„ì¬: {ratio:.2f}, ëª©í‘œ: 1.7~2.3)")
        
        if ratio > target_ratio + tolerance:
            # êµ¬ê°€ ë¶€ì¡±
            needed_phrases = int(total_clause / target_ratio) - llm_phrase_count
            report.append(f"ğŸ“ [ìˆ˜ì • ì§€ì‹œ] êµ¬(ëª…ì‚¬êµ¬/ë™ì‚¬êµ¬/ë¶€ì‚¬êµ¬) {needed_phrases}ê°œ ì¶”ê°€ í•„ìš”")
            report.append(f"   â†’ ë³µí•© ëª…ì‚¬êµ¬ ì‚¬ìš© ê¶Œì¥: 'ì—´ì—­í•™ì  í‰í˜• ìƒíƒœ', 'ì—ë„ˆì§€ ë³´ì¡´ì˜ ì›ë¦¬' ë“±")
        else:
            # êµ¬ê°€ ê³¼ë‹¤
            excess_phrases = llm_phrase_count - int(total_clause / target_ratio)
            report.append(f"ğŸ“ [ìˆ˜ì • ì§€ì‹œ] êµ¬ {excess_phrases}ê°œ ì¶•ì†Œ ë˜ëŠ” ì ˆë¡œ í™•ì¥ í•„ìš”")
            report.append(f"   â†’ ì¼ë¶€ êµ¬ë¥¼ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì „ê°œ ê¶Œì¥")
    
    return "\n".join(report)


def _audit_image_placement(content: str, persona_json: dict = None) -> str:
    """
    ì´ë¯¸ì§€ ë°°ì¹˜ ì ì ˆì„± ê²€ì¦:
    1. ìµœì†Œ ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸
    2. í…ìŠ¤íŠ¸ ë¸”ë¡ ê°„ ì´ë¯¸ì§€ ë¶„í¬ í™•ì¸
    3. alt í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦
    4. ìº¡ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    
    Args:
        content: HTML ì½˜í…ì¸ 
        persona_json: í˜ë¥´ì†Œë‚˜ ì •ë³´ (optional)
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë³´ê³ ì„œ
    """
    soup = BeautifulSoup(content, 'html.parser')
    report = []
    errors = 0
    warnings = 0
    
    report.append("=== ì´ë¯¸ì§€ ë°°ì¹˜ ê²€ì¦ (Image Placement Audit) ===")
    
    # 1. ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸
    images = soup.find_all('img')
    image_count = len(images)
    
    report.append(f"ğŸ“Š [ì´ë¯¸ì§€ ìˆ˜] ì´ {image_count}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
    
    if image_count == 0:
        report.append("âŒ [ì´ë¯¸ì§€-í•„ìˆ˜] ì´ë¯¸ì§€ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 1ê°œ ì´ìƒ ì‚½ì… í•„ìš”.")
        errors += 1
    
    # 2. í…ìŠ¤íŠ¸ ë¸”ë¡ ê°„ ì´ë¯¸ì§€ ë¶„í¬ í™•ì¸
    # ì—°ì† 3ë‹¨ë½ ì´ìƒ í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ì‚½ì… ê¶Œì¥
    paragraphs = soup.find_all('p')
    consecutive_text_blocks = 0
    max_consecutive = 0
    long_text_sections = []
    
    for i, element in enumerate(soup.find_all(['p', 'img', 'table'])):
        if element.name == 'p':
            # í…Œì´ë¸” ë‚´ë¶€ pëŠ” ì œì™¸
            if not element.find_parent('table'):
                consecutive_text_blocks += 1
                max_consecutive = max(max_consecutive, consecutive_text_blocks)
        elif element.name in ['img', 'table']:
            # ì´ë¯¸ì§€ë‚˜ í…Œì´ë¸”(ì´ë¯¸ì§€ í¬í•¨ ê°€ëŠ¥)ì„ ë§Œë‚˜ë©´ ë¦¬ì…‹
            if consecutive_text_blocks >= 4:
                long_text_sections.append(consecutive_text_blocks)
            consecutive_text_blocks = 0
    
    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²´í¬
    if consecutive_text_blocks >= 4:
        long_text_sections.append(consecutive_text_blocks)
    
    if long_text_sections:
        report.append(f"âš ï¸ [ì´ë¯¸ì§€-ë¶„í¬] ì—°ì† í…ìŠ¤íŠ¸ ë¸”ë¡ì´ ê¸´ êµ¬ê°„ ë°œê²¬: {long_text_sections}ê°œ ë‹¨ë½ ì—°ì†")
        report.append(f"   â†’ 3-4ë‹¨ë½ë§ˆë‹¤ ì´ë¯¸ì§€/ì‹œê° ìš”ì†Œ ì‚½ì… ê¶Œì¥")
        warnings += 1
    else:
        report.append("âœ… [ì´ë¯¸ì§€-ë¶„í¬] ì´ë¯¸ì§€ê°€ ì ì ˆíˆ ë¶„í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    # 3. ì„¹ì…˜ë‹¹ ì´ë¯¸ì§€ ë¹„ìœ¨ í™•ì¸
    headings = soup.find_all(['h1', 'h2', 'h3'])
    section_count = max(len(headings), 1)
    
    if image_count > 0:
        images_per_section = image_count / section_count
        if images_per_section < 0.5:
            report.append(f"âš ï¸ [ì´ë¯¸ì§€-ë°€ë„] ì„¹ì…˜ë‹¹ ì´ë¯¸ì§€ ë¹„ìœ¨ ë‚®ìŒ ({images_per_section:.1f}ê°œ/ì„¹ì…˜)")
            report.append(f"   â†’ ì„¹ì…˜ë‹¹ 1-2ê°œ ì´ë¯¸ì§€ ê¶Œì¥")
            warnings += 1
        else:
            report.append(f"âœ… [ì´ë¯¸ì§€-ë°€ë„] ì„¹ì…˜ë‹¹ ì´ë¯¸ì§€ ë¹„ìœ¨ ì ì ˆ ({images_per_section:.1f}ê°œ/ì„¹ì…˜)")
    
    # 4. ê° ì´ë¯¸ì§€ì˜ í’ˆì§ˆ ê²€ì¦
    for idx, img in enumerate(images):
        img_issues = []
        
        # alt í…ìŠ¤íŠ¸ í™•ì¸
        alt = img.get('alt', '')
        if not alt:
            img_issues.append("alt í…ìŠ¤íŠ¸ ëˆ„ë½")
        elif len(alt) < 5:
            img_issues.append(f"alt í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ ('{alt}')")
        elif alt in ['image', 'img', 'ì´ë¯¸ì§€', 'photo', 'ì‚¬ì§„']:
            img_issues.append(f"alt í…ìŠ¤íŠ¸ê°€ ì˜ë¯¸ì—†ìŒ ('{alt}')")
        
        # src í™•ì¸
        src = img.get('src', '')
        if not src:
            img_issues.append("src ì†ì„± ëˆ„ë½")
        elif 'WAITING_FOR_SEARCH' in src or 'placeholder' in src.lower():
            img_issues.append("ì´ë¯¸ì§€ URLì´ í”Œë ˆì´ìŠ¤í™€ë”ì„")
        elif 'unsplash.com' in src.lower() or 'cdn' in src.lower():
            img_issues.append("Unsplash/CDN ì‚¬ìš© ê¸ˆì§€ - GitHub raw URL ì‚¬ìš© í•„ìš”")
        elif 'raw.githubusercontent.com' not in src and not src.startswith('data:'):
            img_issues.append(f"GitHub raw URLì´ ì•„ë‹˜ - í˜•ì‹: https://raw.githubusercontent.com/[user]/[repo]/main/images/[file].png")
        
        # í…Œì´ë¸” ë˜í•‘ í™•ì¸
        parent_table = img.find_parent('table')
        if not parent_table:
            img_issues.append("í…Œì´ë¸”ë¡œ ë˜í•‘ë˜ì§€ ì•ŠìŒ")
        
        # width ìŠ¤íƒ€ì¼ í™•ì¸
        style = img.get('style', '')
        if 'width: 100%' not in style and 'width:100%' not in style:
            img_issues.append("width: 100% ìŠ¤íƒ€ì¼ ëˆ„ë½")
        
        # ìº¡ì…˜ í™•ì¸ (ì´ë¯¸ì§€ ë‹¤ìŒì˜ p íƒœê·¸ ë˜ëŠ” ê°™ì€ td ë‚´ p íƒœê·¸)
        has_caption = False
        if parent_table:
            caption_p = parent_table.find('p')
            if caption_p and len(caption_p.get_text(strip=True)) > 0:
                has_caption = True
        
        if not has_caption:
            img_issues.append("ìº¡ì…˜ ì—†ìŒ")
        
        # ì´ìŠˆ ë¦¬í¬íŠ¸
        if img_issues:
            report.append(f"âŒ [ì´ë¯¸ì§€ #{idx+1}] ë¬¸ì œ ë°œê²¬: {', '.join(img_issues)}")
            errors += 1
        else:
            report.append(f"âœ… [ì´ë¯¸ì§€ #{idx+1}] ëª¨ë“  ê²€ì¦ í†µê³¼")
    
    # 5. ì´ë¯¸ì§€ ì „ëµ ê°€ì´ë“œ (persona ê¸°ë°˜)
    if persona_json and isinstance(persona_json, dict):
        image_strategy = persona_json.get('writer', {}).get('image_strategy', '')
        if image_strategy:
            report.append(f"\nğŸ’¡ [ì´ë¯¸ì§€-ì „ëµ] í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ê¶Œì¥ ìŠ¤íƒ€ì¼: {image_strategy}")
    
    # 6. ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ (2026-01-09 ì‹ ê·œ)
    report.append("\nğŸ“¸ [ì´ë¯¸ì§€-ìŠ¤íƒ€ì¼] í•„ìˆ˜ ê·œì¹™:")
    report.append("   1. âœ… ì‹¤ì‚¬ ì´ë¯¸ì§€ë§Œ ì‚¬ìš© (ì¼ëŸ¬ìŠ¤íŠ¸/ë‹¤ì´ì–´ê·¸ë¨ ê¸ˆì§€)")
    report.append("   2. âœ… í…ìŠ¤íŠ¸ ë¼ë²¨ í•„ìš”ì‹œ â†’ ë””ì§€í„¸ ì†ê¸€ì”¨ ìŠ¤íƒ€ì¼")
    report.append("   3. âœ… GitHub raw URL ì‚¬ìš©: https://raw.githubusercontent.com/[user]/[repo]/main/images/[file].png")
    report.append("   4. âŒ Unsplash/ì™¸ë¶€ CDN ê¸ˆì§€")
    
    # ì¢…í•© ê²°ê³¼
    report.append(f"\nğŸ“Š [ì¢…í•©] ì˜¤ë¥˜: {errors}ê°œ, ê²½ê³ : {warnings}ê°œ")
    
    if errors > 0:
        report.append("âŒ [ì´ë¯¸ì§€ ê°ë…] ì´ë¯¸ì§€ ë°°ì¹˜ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ìœ„ ì‚¬í•­ì„ ìˆ˜ì •í•˜ì„¸ìš”.")
    elif warnings > 0:
        report.append("âš ï¸ [ì´ë¯¸ì§€ ê°ë…] ê¶Œì¥ì‚¬í•­ì„ ê²€í† í•˜ì„¸ìš”.")
    else:
        report.append("âœ… [ì´ë¯¸ì§€ ê°ë…] ì´ë¯¸ì§€ ë°°ì¹˜ê°€ ì ì ˆí•©ë‹ˆë‹¤.")
    
    return "\n".join(report)


def _audit_design_and_image(content: str, persona_json: str) -> str:
    """
    Audits Design Ratio (60:30:10), Image Tone, and Mobile Compatibility.
    """
    soup = BeautifulSoup(content, 'html.parser')
    report = []
    errors = 0
    
    # A. Design Ratio (60:30:10)
    # 60% (Body + Whitespace): <p> tags excluding text in boxes (tables)
    # 30% (Structure): h1-h3, table content, ul/ol
    # 10% (Highlight): strong, span with color
    
    body_text_len = 0
    structure_text_len = 0
    highlight_text_len = 0
    
    # Calculate Body (p tags not inside tables)
    for p in soup.find_all('p'):
        if not p.find_parent('table'):
            body_text_len += len(p.get_text(strip=True))
            
    # Calculate Structure (Headings, Tables, Lists)
    for tag in soup.find_all(['h1', 'h2', 'h3', 'ul', 'ol', 'table']):
         structure_text_len += len(tag.get_text(strip=True))
         
    # Calculate Highlight (Strong, Color Spans)
    for tag in soup.find_all(['strong', 'mark']):
        highlight_text_len += len(tag.get_text(strip=True))
    for span in soup.find_all('span'):
        if 'color' in span.get('style', ''):
            highlight_text_len += len(span.get_text(strip=True))
            
    # Normalize (Structure contains Highlight text usually, avoiding double counting is hard without complex logic)
    # Simple approx: Total = Body + Structure (Structure usually includes everything else not in bare p)
    # Wait, the rule is 60:30:10.
    total = body_text_len + structure_text_len
    
    if total == 0:
         return "âŒ [Design] Content is empty."

    # Adjusting logic: Structure length probably overlaps with Highlight.
    # Let's assume Body is bare text. Structure is Box/Heading text. Highlight is subset.
    # Ratio Calculation:
    # Body Ratio = Body / Total
    # Structure Ratio = Structure / Total
    # Highlight Ratio = Highlight / Total (This is independent, can overlap)
    
    body_ratio = body_text_len / total
    
    if 0.55 <= body_ratio <= 0.65:
        report.append(f"âœ… [Ratio] Body+Whitespace ratio is good ({body_ratio:.1%}).")
    else:
        report.append(f"âŒ [Ratio] Body+Whitespace ratio is {body_ratio:.1%} (Target: 60%). Increase text content or whitespace, reduce emphasis.")
        errors += 1
        
    # B. Image Tone & Format
    images = soup.find_all('img')
    for idx, img in enumerate(images):
        # Format Check
        parent_table = img.find_parent('table')
        if not parent_table:
            report.append(f"âŒ [Image] Image {idx+1} is NOT wrapped in a Table.")
            errors += 1
            
        style = img.get('style', '')
        if 'width: 100%' not in style and 'width:100%' not in style:
            report.append(f"âŒ [Image] Image {idx+1} missing 'width: 100%' style for mobile.")
            errors += 1
            
        # Tone Check (Caption)
        alt = img.get('alt', '')
        if not alt:
            report.append(f"âŒ [Image] Image {idx+1} missing 'alt' text.")
            errors += 1
            
    # C. No Emoji
    text_all = soup.get_text()
    if re.search(r'[^\x00-\x7Fê°€-í£]', text_all): # Rough check for non-ascii/non-korean (emojis)
        # Note: This regex is too broad, might catch punctuation.
        # Better: use explicit ranges or a library.
        # Simple Emoji Regex
        emoji_pattern = re.compile(r'[\U00010000-\U0010ffff]', flags=re.UNICODE)
        if emoji_pattern.search(text_all):
             report.append(f"âŒ [Emoji] Emojis found. Remove all emojis.")
             errors += 1
             
    # Mobile Compatibility
    # Check Line Height
    p_tags = soup.find_all('p')
    for p in p_tags:
        style = p.get('style', '')
        if 'line-height' not in style: # Strict check
             pass # Might be inherited? Plan says "Every p tag must have line-height: 1.8"
             # Let's inspect a sample
    
    return "\n".join(report)

def _validate_technical_constraints(content: str) -> str:
    """
    Validates Naver Blog HTML constraints (Block List).
    """
    report = []
    errors = 0
    
    forbidden = [
        ('<style>', r'<style'),
        ('class=', r'class='),
        ('id=', r'\bid='),
        ('border-radius', r'border-radius'),
        ('linear-gradient', r'linear-gradient'),
        ('max-width', r'max-width'),
        ('box-shadow', r'box-shadow'),
        ('display: flex', r'display:\s*flex'),
        ('display: grid', r'display:\s*grid')
    ]
    
    for name, pattern in forbidden:
        if re.search(pattern, content, re.IGNORECASE):
            report.append(f"âŒ [Tech] Forbidden element found: '{name}'.")
            errors += 1
            
    if errors == 0:
        report.append("âœ… [Tech] All technical constraints passed.")
        
    return "\n".join(report)

# -----------------------------------------------------------------------------
# MCP Tool Decorators
# -----------------------------------------------------------------------------

@mcp.tool()
def generate_personas(input_text: str) -> str:
    return _generate_personas(input_text)

@mcp.tool()
def audit_linguistic_quality(content: str, persona_json: str) -> str:
    return _audit_linguistic_quality(content, persona_json)

@mcp.tool()
def audit_design_and_image(content: str, persona_json: str) -> str:
    return _audit_design_and_image(content, persona_json)

@mcp.tool()
def validate_technical_constraints(content: str) -> str:
    return _validate_technical_constraints(content)

@mcp.tool()
def audit_image_placement(content: str, persona_json: str = "{}") -> str:
    """Audit image placement, distribution, and quality."""
    import json
    persona = json.loads(persona_json) if isinstance(persona_json, str) else persona_json
    return _audit_image_placement(content, persona)

def _generate_blog_tags(content: str, persona_json: dict = None) -> str:
    """
    ì½˜í…ì¸ ì™€ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ìœ¼ë¡œ ë¸”ë¡œê·¸ íƒœê·¸ ì¶”ì²œ (2ê´€ì ).
    
    âš ï¸ ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” LLMì—ê²Œ ê°€ì´ë“œë¼ì¸ë§Œ ì œê³µí•©ë‹ˆë‹¤.
    ì‹¤ì œ íƒœê·¸ ìƒì„±ì€ LLMì´ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
    
    Args:
        content: HTML ì½˜í…ì¸ 
        persona_json: í˜ë¥´ì†Œë‚˜ ì •ë³´
    
    Returns:
        íƒœê·¸ ì¶”ì²œ ê°€ì´ë“œë¼ì¸ ë° í”„ë¡¬í”„íŠ¸
    """
    soup = BeautifulSoup(content, 'html.parser')
    text_content = soup.get_text()
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë° ë³µì¡ë„ ë¶„ì„ (ë©”íƒ€ ì •ë³´ë§Œ)
    word_count = len(text_content)
    
    report = []
    report.append("=== ğŸ·ï¸  ë¸”ë¡œê·¸ íƒœê·¸ ì¶”ì²œ (LLM Auto-Generation) ===\n")
    report.append(f"ğŸ“Š ì½˜í…ì¸  ë©”íƒ€ì •ë³´: {word_count}ì\n")
    
    # LLMì—ê²Œ ì œê³µí•  íƒœê·¸ ìƒì„± ê°€ì´ë“œë¼ì¸
    report.append("=" * 60)
    report.append("ğŸ’¡ **LLM íƒœê·¸ ìƒì„± ì§€ì¹¨ (Two-Perspective Approach)**")
    report.append("=" * 60)
    report.append("")
    
    report.append("ğŸ“Š **ê´€ì  1: ì „ë¬¸ê°€ ì§€í–¥ íƒœê·¸ (Expert-Oriented)**")
    report.append("   ëª©í‘œ: ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì¸ë±ì‹±, í•™ìˆ /ì „ë¬¸ê°€ ê²€ìƒ‰ ìœ ì…")
    report.append("   ìƒì„± ì›ì¹™:")
    report.append("   â€¢ ì˜ë¬¸ í•™ìˆ  ìš©ì–´ ì‚¬ìš© (ì˜ˆ: Maxwell_relations, thermodynamics)")
    report.append("   â€¢ ëª…í™•í•œ ê¸°ìˆ  í‚¤ì›Œë“œ (ì˜ˆ: Gibbs_free_energy, phase_transition)")
    report.append("   â€¢ ê²€ìƒ‰ ì—”ì§„ ìµœì í™” (SEO) ê³ ë ¤")
    report.append("   â€¢ ë¡±í…Œì¼ í‚¤ì›Œë“œ ì „ëµ (ì˜ˆ: Clausius_Clapeyron_equation)")
    report.append("   **ìƒì„±í•  íƒœê·¸ ìˆ˜: 5-6ê°œ**")
    report.append("")
    
    report.append("ğŸ¯ **ê´€ì  2: ì´ˆë³´ì ì§€í–¥ íƒœê·¸ (Beginner-Oriented)**")
    report.append("   ëª©í‘œ: ì¼ë°˜ì¸ ê²€ìƒ‰ ìœ ì…, í•™ìŠµì/ì…ë¬¸ì íƒ€ê²ŸíŒ…")
    report.append("   ìƒì„± ì›ì¹™:")
    report.append("   â€¢ í•œê¸€ ì‰¬ìš´ í‘œí˜„ (ì˜ˆ: ì—´ì—­í•™_ì‰½ê²Œ, ë¬¼ë¦¬_ê¸°ì´ˆ)")
    report.append("   â€¢ 'ë€', 'ì´ë€', '_ì…ë¬¸', '_ì‰½ê²Œ' ì ‘ë¯¸ì‚¬ í™œìš©")
    report.append("   â€¢ ëŒ€ì¤‘ì  ê²€ìƒ‰ì–´ (ì˜ˆ: ê³¼í•™_ê³µë¶€, ëŒ€í•™ë¬¼ë¦¬)")
    report.append("   â€¢ êµìœ¡/í•™ìŠµ ê´€ë ¨ í‚¤ì›Œë“œ (ì˜ˆ: ë…í•™, ê°œë…ì •ë¦¬)")
    report.append("   **ìƒì„±í•  íƒœê·¸ ìˆ˜: 5-6ê°œ**")
    report.append("")
    
    report.append("ğŸ”§ **íƒœê·¸ ìƒì„± í”„ë¡œì„¸ìŠ¤:**")
    report.append("   1. ì½˜í…ì¸ ì˜ í•µì‹¬ ì£¼ì œì™€ í‚¤ì›Œë“œë¥¼ ììœ¨ ë¶„ì„")
    report.append("   2. ê´€ì  1: ì „ë¬¸ ìš©ì–´ ê¸°ë°˜ íƒœê·¸ 5-6ê°œ ìƒì„±")
    report.append("   3. ê´€ì  2: ëŒ€ì¤‘ ê²€ìƒ‰ì–´ ê¸°ë°˜ íƒœê·¸ 5-6ê°œ ìƒì„±")
    report.append("   4. ì¤‘ë³µ ì œê±° ë° ê²€ìƒ‰ íš¨ìœ¨ì„± ê²€ì¦")
    report.append("   5. ìµœì¢… íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ '#íƒœê·¸ëª…' í˜•ì‹ìœ¼ë¡œ ì¶œë ¥")
    report.append("")
    
    report.append("ğŸ’¡ **ì‚¬ìš© ì „ëµ (ìë™ ì•ˆë‚´):**")
    report.append("   â€¢ ë„¤ì´ë²„ ë¸”ë¡œê·¸: ì´ˆë³´ì íƒœê·¸ 5ê°œ + ì „ë¬¸ê°€ íƒœê·¸ 1-2ê°œ í˜¼í•©")
    report.append("   â€¢ í‹°ìŠ¤í† ë¦¬/ë¸ŒëŸ°ì¹˜: ì „ë¬¸ê°€ íƒœê·¸ ìœ„ì£¼ë¡œ SEO ìµœì í™”")
    report.append("   â€¢ ê²€ìƒ‰ ì—”ì§„ ë…¸ì¶œ: ì „ë¬¸ê°€ íƒœê·¸ë¡œ ë¡±í…Œì¼ í‚¤ì›Œë“œ í™•ë³´")
    report.append("   â€¢ ì†Œì…œ ë¯¸ë””ì–´: ì´ˆë³´ì íƒœê·¸ë¡œ ëŒ€ì¤‘ ì ‘ê·¼ì„± í–¥ìƒ")
    report.append("")
    
    report.append("=" * 60)
    report.append("ğŸ¤– **LLM ì‹¤í–‰ í•„ìš”: ìœ„ ê°€ì´ë“œë¼ì¸ì„ ì½ê³  ì‹¤ì œ íƒœê·¸ë¥¼ ìƒì„±í•˜ì„¸ìš”**")
    report.append("=" * 60)
    
    return "\n".join(report)

@mcp.tool()
def generate_blog_tags(content: str, persona_json: str = "{}") ->str:
    """Generate blog tag recommendations from two perspectives: expert and beginner."""
    import json
    persona = json.loads(persona_json) if isinstance(persona_json, str) else persona_json
    return _generate_blog_tags(content, persona)

if __name__ == "__main__":
    mcp.run()
